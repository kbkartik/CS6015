\documentclass[solution,addpoints,12pt]{exam}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{animate}
\usepackage{hyperref}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}

\newenvironment{Solution}{\begin{EnvFullwidth}\begin{solution}}{\end{solution}\end{EnvFullwidth}}

\printanswers
%\unframedsolutions
\pagestyle{headandfoot}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% INSTRUCTIONS %%%%%%%%%%%%%%%%%%%%%
% * Fill in your name and roll number below

% * Answer in place (after each question)

% * Use \begin{solution} and \end{solution} to typeset
%   your answers.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Fill in the details below
\def\studentName{\textbf{Name: TODO}}
\def\studentRoll{\textbf{Roll No: TODO}}

\firstpageheader{CS 6015 (LARP) - Homework 4 (\numpoints~Marks)}{}{\studentName,\studentRoll}
\firstpageheadrule

\newcommand{\brac}[1]{\left[ #1 \right]}
\newcommand{\curly}[1]{\left\{ #1 \right\}}
\newcommand{\paren}[1]{\left( #1 \right)}
\newcommand{\card}[1]{\left\lvert #1 \right\rvert}

\begin{document}

\noindent \textbf{Honor code}: I pledge on my honor that: I have completed all steps in the below homework on my own, I have not used any unauthorized materials while completing this homework, and I have not given anyone else access to my homework.
\\~\\~\\
\begin{flushright}
\textbf{Name and Signature}

\end{flushright}


\begin{questions}

\question[1] Have you read and understood the honor code?
\begin{solution}

\end{solution}

\uplevel{\textbf{Eigenstory: Special Properties}}

\question[1] Prove that for any square matrix $A$ the eigenvectors corresponding to distinct eigenvalues are always independent. 
\begin{solution}

\end{solution}

\question[2] Prove the following. 
\begin{parts}
\part The sum of the eigenvalues of a matrix is equal to its trace.
\begin{solution}

\end{solution}
\part The product of the eigenvalues of a matrix is equal to its determinant.
\begin{solution}

\end{solution}
\end{parts}

\question[2] What is the relationship between the rank of a matrix and the number of non-zero eigenvalues? Explain your answer.
\begin{solution}
I think the answer to this question is ``The rank of a matrix is equal to the number of non-zero eigenvalues if $\cdots$''
\end{solution}

\question[1] If $A$ is a square symmetric matrix then prove that the number of positive pivots it has is the same as the number of positive eigenvalues it has. 
\begin{solution}

\end{solution}

\uplevel{\textbf{Eigenstory: Special Matrices}}
\question[2] Consider the matrix $R = I - 2\mathbf{u}\mathbf{u}^\top$ where $\mathbf{u}$ is a unit vector $\in \mathbb{R}^n$.

\begin{parts}
\part Show that $R$ is symmetric and orthogonal. (How many independent vectors will $R$ have?)
\begin{solution}

\end{solution}


\part Let $\mathbf{u} = \frac{1}{\sqrt{2}}\begin{bmatrix} 0 \\ 1\\ 1 \end{bmatrix}$. Draw the line passing through this vector in geogebra (or any tool of your choice). Now take any vector in $\mathbf{R}^3$ and multiply it with the matrix $R$ (i.e., the matrix $R$ as defined above with $\mathbf{u} = \frac{1}{\sqrt{2}}\begin{bmatrix} 0 \\ 1\\ 1 \end{bmatrix}$). What do you observe or what do you think the matrix $R$ does or what would you call matrix $R$? (Hint: the name starts with $R$)
\begin{solution}

\end{solution}

\part Compute the eigenvalues and eigenvectors of the matrix $R$ as defined above with $\mathbf{u} = \frac{1}{\sqrt{2}}\begin{bmatrix} 0 \\ 1\\ 1 \end{bmatrix}$
\begin{solution}

\end{solution}

\part I believe that irrespective of what $\mathbf{u}$ is any such matrix $R$ will have the same eigenvalues as you obtained above (with one of the eigenvalues repeating). Can you reason why this is the case? (Hint: think about how we reasoned about the eigenvectors of the projection matrix $P$ even without computing them.)
\begin{solution}

\end{solution}

\end{parts}

\question[2] Let $Q$ be a $n \times n$ real orthogonal matrix (i.e., all its elements are real and its columns are orthonormal). State with reason whether the following statements are True or False (provide a proof if the statement is True and a counter-example if it is False).

\begin{parts}
\part If $\lambda$ is an eigenvalue of $Q$ then $|\lambda| = 1$
\begin{solution}

\end{solution}
\part The eigenvectors of $Q$ are orthogonal 
\begin{solution}

\end{solution}
\part $Q$ is always diagonalizable.
\begin{solution}

\end{solution}
\end{parts}

\question[1\half] Any rank one matrix can be written as $\mathbf{u}\mathbf{v}^\top$. 
\begin{parts}
\part Prove that the eigenvalues of any rank one matrix are $\mathbf{v}^\top\mathbf{u}$ and $0$.
\begin{solution}

\end{solution}

\part How many times does the value $0$ repeat? 
\begin{solution}

\end{solution}

\part What are the eigenvectors corresponding to these eigenvalues?
\begin{solution}

\end{solution}
\end{parts}


\question[2] Consider a $n \times n$ Markov matrix.
\begin{parts}
\part Prove that the dominant eigenvalue of a Markov matrix is 1
\begin{solution}
\\
Proof (part 1): 1 is an eigenvalue of a Markov matrix \\
Proof (part 2): all other eigenvalues are less than 1\\
(If you have a simpler way of proving this instead of proving it in two parts then feel free to do so but your proof should convince me about both these parts.)
\end{solution}

\part Consider any $2\times2$ matrix $\begin{bmatrix}a & b \\ c& d \end{bmatrix}$ such that $a + b = c + d$. Show that one of the eigenvalues of such a matrix is 1. (I hope you notice that a Markov matrix is a special case of such a matrix where  $a + b = c + d = 1$.)
\begin{solution}
\end{solution}

\part Does the result extend to $n \times n$ matrices where the sum of the elements of a row is the same for all the $n$ rows? (Explain with reason)
\begin{solution}

\end{solution}

\part What is the corresponding eigenvector?
\begin{solution}

\end{solution}

\end{parts}


\uplevel{\textbf{Eigenstory: Special Relations}}

\question[4] For each of the statements below state True or False with reason.

\begin{parts}
\part The eigenvalues of $A^T$ are \textbf{always} the same as that of $A$.
\begin{solution}

\end{solution}
\part The eigenvectors of $A^T$ are \textbf{always} the same as that of $A$
\begin{solution}

\end{solution}
\part The eigenvalues of $A^{-1}$ are \textbf{always} the reciprocal of  the eigenvalues of $A$.
\begin{solution}

\end{solution}
\part The eigenvectors of $A^{-1}$ are \textbf{always} the same as  the eigenvectors of $A$.
\begin{solution}

\end{solution}
\part If $\mathbf{x}$ is an eigenvector of $A$ and $B$ then it is also an eigenvector of both $AB$ and $BA$, even if the eigenvalues of $A$ and $B$ corresponding to $\mathbf{x}$ are different.
\begin{solution}

\end{solution}
\part If $\mathbf{x}$ is and eigenvector of $A$ and $B$ then it is also an eigenvector of $A+B$
\begin{solution}

\end{solution}
\part If $\lambda$ is an eigenvalue of $A$ then $\lambda + k$ is an eigenvalue of $A + kI$.
\begin{solution}

\end{solution}

\part The non-zero eigenvalues of $AA^\top$ and $A^\top A$ are equal.
\begin{solution}
~\\
\end{solution}
\end{parts}

\uplevel{\textbf{Eigenstory: Change of basis}} 

\question[2] Consider the following two basis. Basis 1: $\mathbf{u_1} = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 1 \end{bmatrix}, \mathbf{u_2} = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ -1 \end{bmatrix},$ and Basis 2: $\mathbf{u_1} = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ -1 \end{bmatrix}, \mathbf{u_2} = \frac{1}{\sqrt{2}}\begin{bmatrix} -1 \\ -1 \end{bmatrix},$. Consider a vector $\mathbf{x} = \begin{bmatrix} a \\ b \end{bmatrix}$ in Basis 1 (i.e., $\mathbf{x} = a \mathbf{u_1} + b \mathbf{u_2}$). How would you represent it in Basis 2?
\begin{solution}

\end{solution}

\question[1] Let $\mathbf{u}$ and $\mathbf{v}$ be two vectors in the standard basis. Let $T(\mathbf{u})$ and $T(\mathbf{v})$ be the representation of these vectors in a different basis. Prove that $\mathbf{u}\cdot\mathbf{v} = T(\mathbf{u})\cdot T(\mathbf{v})$ if and only if the basis represented by $T$ is an orthonormal basis (i.e., dot products are preserved only when the new basis is orthonormal).
\begin{solution}

\end{solution}

\uplevel{\textbf{Eigenstory: PCA and SVD}}
\question[1] How are PCA and SVD related? (no vague answers please, think and answer very precisely with mathematical reasoning)
\begin{solution}

\end{solution}

\question[1\half] Consider the matrix $\begin{bmatrix} 4 & 4 \\ -3 & 3 \end{bmatrix}$

\begin{parts}
\part Find $\Sigma$ and $V$, \textit{i.e.}, the eigenvalues and eigenvectors of $A^\top A$
\begin{solution}

\end{solution}
\part Find $\Sigma$ and $U$, \textit{i.e.}, the eigenvalues and eigenvectors of $A A^\top$
\begin{solution}

\end{solution}
\part Now compute $U\Sigma V^\top$. Did you get back $A$? If yes, good! If not, what went wrong?
\begin{solution}
Please refer to following lectures of Prof. Gilbert Strang to understand what went wrong and then correct your answer (if it was wrong): 
\begin{itemize}
    \item \url{https://www.youtube.com/watch?v=TX_vooSnhm8&t=1177s} (starts at 1177 seconds)
    \item \url{https://www.youtube.com/watch?v=HgC1l_6ySkc&feature=youtu.be&t=1731)} (starts at 1731 seconds)
\end{itemize}
\end{solution}

\end{parts}

\question[2] Prove that the matrices $U$ and $V$ that you get from the SVD of a matrix $A$ contain the basis vectors for the four fundamental subspaces of $A$. (this is where the whole course comes together: fundamental subspaces, basis vectors, orthonormal vectors,  eigenvectors, and our special symmetric matrices $AA^\top$, $A^\top A$!)
\begin{solution}

\end{solution}

\question[2] Fun with flags.

\begin{parts}
\part Browse through the flags of all countries and paste 5 rank one flags below.
\begin{solution}

\end{solution}
\part What is the rank of the flag of Greece?
\begin{solution}

\end{solution}
\end{parts}

\question[2] Consider the LFW dataset (Labeled Faces in the Wild). 

\begin{parts}
\part Perform PCA using this dataset and plot the first 25 eigenfaces (in a $5 \times 5$ grid) 

\begin{solution}
Here is something to get you started.

\begin{verbatim}
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_lfw_people
from sklearn.decomposition import PCA


# Load data
lfw_dataset = fetch_lfw_people(min_faces_per_person=100)

_, h, w = lfw_dataset.images.shape
X = lfw_dataset.data

# Compute a PCA 
n_components = 100
pca = PCA(n_components=n_components, whiten=True).fit(X)

Beyond this you are on your own. Good Luck!
\end{verbatim}
\end{solution}

\part Take your close-up photograph (face only) and reconstruct it using the first 25 eigenfaces :-). If due to privacy concerns, you do not want to to use your own photo then feel free to use a publicly available close-up photo (face only) of your favorite celebrity.

\begin{solution}

\end{solution}

\end{parts}

~\\~\\...And that concludes the story of \textit{How I Met Your Eigenvectors :-)} (I hope you enjoyed it!)

\end{questions}
\end{document} 